{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from IPython.display import display\n",
    "import re\n",
    "import os\n",
    "import json\n",
    "import gzip\n",
    "\n",
    "from SocialMediaIE.data.tokenization import tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "SENTIMENT_WRITE_DIR = \"../data/processed/SENTIMENT\"\n",
    "ABUSIVE_WRITE_DIR = \"../data/processed/ABUSIVE\"\n",
    "UNCERTAINITY_WRITE_DIR = \"../data/processed/UNCERTAINITY\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "! rm -rf ../data/processed/SENTIMENT\n",
    "! rm -rf ../data/processed/ABUSIVE\n",
    "! rm -rf ../data/processed/UNCERTAINITY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_tweet_json_file(path):\n",
    "    open_fn = open\n",
    "    if path.endswith(\".gz\"):\n",
    "        open_fn = gzip.open\n",
    "    with open_fn(path, \"rt\", encoding=\"utf-8\") as fp:\n",
    "        for line in fp:\n",
    "            row = json.loads(line)\n",
    "            yield row\n",
    "            \n",
    "def tweet_json_to_df(json_file_path):\n",
    "    df_json = pd.DataFrame([\n",
    "        (row[\"id\"], row[\"full_text\"]) \n",
    "        for row in read_tweet_json_file(json_file_path)\n",
    "    ], columns=[\"tweet_id\", \"text\"]).drop_duplicates(subset=\"tweet_id\")\n",
    "    return df_json\n",
    "            \n",
    "def write_data(df, base_dir):\n",
    "    groups = df.groupby([\"dataset\", \"datasplit\"])\n",
    "    for (dataset, datasplit), df_group in groups:\n",
    "        output_dir = os.path.join(base_dir, dataset)\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        output_file = os.path.join(output_dir, f\"{datasplit}.json\")\n",
    "        print(f\"Data: {dataset}, split: {datasplit}, shape: {df_group.shape}, written to {output_file}\")\n",
    "        df_group[[\"tweet_id\", \"text\", \"label\"]].to_json(output_file, orient=\"records\", lines=True)\n",
    "        \n",
    "        \n",
    "def split_data(df):\n",
    "    df_train, df_test = train_test_split(\n",
    "        df, test_size=0.2, random_state=1337, stratify=df.label)\n",
    "    df_train, df_dev = train_test_split(\n",
    "        df_train, test_size=0.1, random_state=1337, stratify=df_train.label)\n",
    "    df.loc[df_train.index, \"datasplit\"] = \"train\"\n",
    "    df.loc[df_dev.index, \"datasplit\"] = \"dev\"\n",
    "    df.loc[df_test.index, \"datasplit\"] = \"test\"\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parse Sentiment Benchmark Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_sentiment_data(base_dir):\n",
    "    split_file_path = os.path.join(base_dir, \"data_with_train_dev_test_split.txt\")\n",
    "    json_file_path = os.path.join(base_dir, \"joined_data_all.text.json\")\n",
    "    df = pd.read_csv(split_file_path, sep=\"\\t\")\n",
    "    df = df[(df.language == \"english\")]\n",
    "    df_json = pd.read_json(json_file_path, orient=\"split\").drop_duplicates(subset=\"tid\")\n",
    "    df = df.merge(df_json, on=\"tid\", how=\"inner\")\n",
    "    df = df.rename(columns={\"tid\": \"tweet_id\"})\n",
    "    return df\n",
    "\n",
    "SENTIMENT_BASE_DIR = \"G:\\\\AzureBackup\\\\Datasets\\\\Twitter\\\\TwitterSentimentBenchmarks\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['tweet_id', 'favorites', 'is_quote', 'is_reply', 'retweets', 'u_id',\n",
      "       'u_created_at', 'u_followers', 'u_friends', 'u_lists', 'u_statuses',\n",
      "       'u_verified', 'u_location', 'u_has_url', 'n_hashtags', 'n_symbols',\n",
      "       'n_urls', 'n_mentions', 'created_at', 'dataset', 'datasplit',\n",
      "       'language', 'label', 'text'],\n",
      "      dtype='object')\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr:last-of-type th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th>datasplit</th>\n",
       "      <th colspan=\"3\" halign=\"left\">dev</th>\n",
       "      <th colspan=\"3\" halign=\"left\">test</th>\n",
       "      <th colspan=\"3\" halign=\"left\">train</th>\n",
       "      <th>All</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>label</th>\n",
       "      <th>negative</th>\n",
       "      <th>neutral</th>\n",
       "      <th>positive</th>\n",
       "      <th>negative</th>\n",
       "      <th>neutral</th>\n",
       "      <th>positive</th>\n",
       "      <th>negative</th>\n",
       "      <th>neutral</th>\n",
       "      <th>positive</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dataset</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Airline</th>\n",
       "      <td>613</td>\n",
       "      <td>205</td>\n",
       "      <td>163</td>\n",
       "      <td>1532</td>\n",
       "      <td>512</td>\n",
       "      <td>408</td>\n",
       "      <td>5515</td>\n",
       "      <td>1843</td>\n",
       "      <td>1467</td>\n",
       "      <td>12258</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Clarin</th>\n",
       "      <td>1276</td>\n",
       "      <td>2158</td>\n",
       "      <td>1500</td>\n",
       "      <td>3191</td>\n",
       "      <td>5394</td>\n",
       "      <td>3749</td>\n",
       "      <td>11485</td>\n",
       "      <td>19418</td>\n",
       "      <td>13496</td>\n",
       "      <td>61667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GOP</th>\n",
       "      <td>471</td>\n",
       "      <td>202</td>\n",
       "      <td>130</td>\n",
       "      <td>1175</td>\n",
       "      <td>505</td>\n",
       "      <td>326</td>\n",
       "      <td>4230</td>\n",
       "      <td>1818</td>\n",
       "      <td>1173</td>\n",
       "      <td>10030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Healthcare</th>\n",
       "      <td>405</td>\n",
       "      <td>178</td>\n",
       "      <td>141</td>\n",
       "      <td>428</td>\n",
       "      <td>156</td>\n",
       "      <td>133</td>\n",
       "      <td>326</td>\n",
       "      <td>192</td>\n",
       "      <td>172</td>\n",
       "      <td>2131</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Obama</th>\n",
       "      <td>80</td>\n",
       "      <td>79</td>\n",
       "      <td>50</td>\n",
       "      <td>199</td>\n",
       "      <td>197</td>\n",
       "      <td>126</td>\n",
       "      <td>715</td>\n",
       "      <td>707</td>\n",
       "      <td>455</td>\n",
       "      <td>2608</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SemEval</th>\n",
       "      <td>836</td>\n",
       "      <td>1779</td>\n",
       "      <td>1968</td>\n",
       "      <td>3418</td>\n",
       "      <td>11097</td>\n",
       "      <td>8588</td>\n",
       "      <td>1736</td>\n",
       "      <td>5223</td>\n",
       "      <td>5286</td>\n",
       "      <td>39931</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>All</th>\n",
       "      <td>3681</td>\n",
       "      <td>4601</td>\n",
       "      <td>3952</td>\n",
       "      <td>9943</td>\n",
       "      <td>17861</td>\n",
       "      <td>13330</td>\n",
       "      <td>24007</td>\n",
       "      <td>29201</td>\n",
       "      <td>22049</td>\n",
       "      <td>128625</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "datasplit       dev                      test                     train  \\\n",
       "label      negative neutral positive negative neutral positive negative   \n",
       "dataset                                                                   \n",
       "Airline         613     205      163     1532     512      408     5515   \n",
       "Clarin         1276    2158     1500     3191    5394     3749    11485   \n",
       "GOP             471     202      130     1175     505      326     4230   \n",
       "Healthcare      405     178      141      428     156      133      326   \n",
       "Obama            80      79       50      199     197      126      715   \n",
       "SemEval         836    1779     1968     3418   11097     8588     1736   \n",
       "All            3681    4601     3952     9943   17861    13330    24007   \n",
       "\n",
       "datasplit                       All  \n",
       "label      neutral positive          \n",
       "dataset                              \n",
       "Airline       1843     1467   12258  \n",
       "Clarin       19418    13496   61667  \n",
       "GOP           1818     1173   10030  \n",
       "Healthcare     192      172    2131  \n",
       "Obama          707      455    2608  \n",
       "SemEval       5223     5286   39931  \n",
       "All          29201    22049  128625  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = load_sentiment_data(SENTIMENT_BASE_DIR)\n",
    "print(df.columns)\n",
    "df.pivot_table(index=\"dataset\", columns=[\"datasplit\", \"label\"], values=\"tweet_id\", aggfunc=len, margins=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data: Airline, split: dev, shape: (981, 24), written to ../data/processed/SENTIMENT\\Airline\\dev.json\n",
      "Data: Airline, split: test, shape: (2452, 24), written to ../data/processed/SENTIMENT\\Airline\\test.json\n",
      "Data: Airline, split: train, shape: (8825, 24), written to ../data/processed/SENTIMENT\\Airline\\train.json\n",
      "Data: Clarin, split: dev, shape: (4934, 24), written to ../data/processed/SENTIMENT\\Clarin\\dev.json\n",
      "Data: Clarin, split: test, shape: (12334, 24), written to ../data/processed/SENTIMENT\\Clarin\\test.json\n",
      "Data: Clarin, split: train, shape: (44399, 24), written to ../data/processed/SENTIMENT\\Clarin\\train.json\n",
      "Data: GOP, split: dev, shape: (803, 24), written to ../data/processed/SENTIMENT\\GOP\\dev.json\n",
      "Data: GOP, split: test, shape: (2006, 24), written to ../data/processed/SENTIMENT\\GOP\\test.json\n",
      "Data: GOP, split: train, shape: (7221, 24), written to ../data/processed/SENTIMENT\\GOP\\train.json\n",
      "Data: Healthcare, split: dev, shape: (724, 24), written to ../data/processed/SENTIMENT\\Healthcare\\dev.json\n",
      "Data: Healthcare, split: test, shape: (717, 24), written to ../data/processed/SENTIMENT\\Healthcare\\test.json\n",
      "Data: Healthcare, split: train, shape: (690, 24), written to ../data/processed/SENTIMENT\\Healthcare\\train.json\n",
      "Data: Obama, split: dev, shape: (209, 24), written to ../data/processed/SENTIMENT\\Obama\\dev.json\n",
      "Data: Obama, split: test, shape: (522, 24), written to ../data/processed/SENTIMENT\\Obama\\test.json\n",
      "Data: Obama, split: train, shape: (1877, 24), written to ../data/processed/SENTIMENT\\Obama\\train.json\n",
      "Data: SemEval, split: dev, shape: (4583, 24), written to ../data/processed/SENTIMENT\\SemEval\\dev.json\n",
      "Data: SemEval, split: test, shape: (23103, 24), written to ../data/processed/SENTIMENT\\SemEval\\test.json\n",
      "Data: SemEval, split: train, shape: (12245, 24), written to ../data/processed/SENTIMENT\\SemEval\\train.json\n"
     ]
    }
   ],
   "source": [
    "write_data(df, SENTIMENT_WRITE_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parse Abusive Benchmark Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_abusive_data(base_dir):\n",
    "    split_file_path = os.path.join(base_dir, \"hatespeechtwitter.tab\")\n",
    "    json_file_path = os.path.join(base_dir, \"tweets.jsonl\")\n",
    "    df = pd.read_csv(split_file_path, sep=\"\\t\")\n",
    "    df = df.rename(columns={\"maj_label\": \"label\"})\n",
    "    print(df.shape)\n",
    "    df_json = tweet_json_to_df(json_file_path)\n",
    "    print(df_json.shape)\n",
    "    df = df.merge(df_json, on=\"tweet_id\", how=\"inner\")\n",
    "    print(df.shape)\n",
    "    df[\"dataset\"] = \"Founta\"\n",
    "    df[\"datasplit\"] = \"train\"\n",
    "    df = split_data(df)\n",
    "    return df\n",
    "\n",
    "ABUSIVE_BASE_DIR = \"G:\\\\AzureBackup\\\\Datasets\\\\Twitter\\\\AbusiveTweets\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(80000, 2)\n",
      "(58222, 2)\n",
      "(58281, 3)\n",
      "Index(['tweet_id', 'label', 'text', 'dataset', 'datasplit'], dtype='object')\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr:last-of-type th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th>datasplit</th>\n",
       "      <th colspan=\"4\" halign=\"left\">dev</th>\n",
       "      <th colspan=\"4\" halign=\"left\">test</th>\n",
       "      <th colspan=\"4\" halign=\"left\">train</th>\n",
       "      <th>All</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>label</th>\n",
       "      <th>abusive</th>\n",
       "      <th>hateful</th>\n",
       "      <th>normal</th>\n",
       "      <th>spam</th>\n",
       "      <th>abusive</th>\n",
       "      <th>hateful</th>\n",
       "      <th>normal</th>\n",
       "      <th>spam</th>\n",
       "      <th>abusive</th>\n",
       "      <th>hateful</th>\n",
       "      <th>normal</th>\n",
       "      <th>spam</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dataset</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Founta</th>\n",
       "      <td>470</td>\n",
       "      <td>186</td>\n",
       "      <td>3274</td>\n",
       "      <td>733</td>\n",
       "      <td>1175</td>\n",
       "      <td>466</td>\n",
       "      <td>8185</td>\n",
       "      <td>1831</td>\n",
       "      <td>4230</td>\n",
       "      <td>1677</td>\n",
       "      <td>29464</td>\n",
       "      <td>6590</td>\n",
       "      <td>58281</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>All</th>\n",
       "      <td>470</td>\n",
       "      <td>186</td>\n",
       "      <td>3274</td>\n",
       "      <td>733</td>\n",
       "      <td>1175</td>\n",
       "      <td>466</td>\n",
       "      <td>8185</td>\n",
       "      <td>1831</td>\n",
       "      <td>4230</td>\n",
       "      <td>1677</td>\n",
       "      <td>29464</td>\n",
       "      <td>6590</td>\n",
       "      <td>58281</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "datasplit     dev                        test                        train  \\\n",
       "label     abusive hateful normal spam abusive hateful normal  spam abusive   \n",
       "dataset                                                                      \n",
       "Founta        470     186   3274  733    1175     466   8185  1831    4230   \n",
       "All           470     186   3274  733    1175     466   8185  1831    4230   \n",
       "\n",
       "datasplit                         All  \n",
       "label     hateful normal  spam         \n",
       "dataset                                \n",
       "Founta       1677  29464  6590  58281  \n",
       "All          1677  29464  6590  58281  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = load_abusive_data(ABUSIVE_BASE_DIR)\n",
    "print(df.columns)\n",
    "df.pivot_table(index=\"dataset\", columns=[\"datasplit\", \"label\"], values=\"tweet_id\", aggfunc=len, margins=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data: Founta, split: dev, shape: (4663, 5), written to ../data/processed/ABUSIVE\\Founta\\dev.json\n",
      "Data: Founta, split: test, shape: (11657, 5), written to ../data/processed/ABUSIVE\\Founta\\test.json\n",
      "Data: Founta, split: train, shape: (41961, 5), written to ../data/processed/ABUSIVE\\Founta\\train.json\n"
     ]
    }
   ],
   "source": [
    "write_data(df, ABUSIVE_WRITE_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parse Abusive HateSpeech data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_hatespeech_srw_data(base_dir):\n",
    "    srw_split_file_path = os.path.join(base_dir, \"NAACL_SRW_2016.csv\")\n",
    "    css_split_file_path = os.path.join(base_dir, \"NLP+CSS_2016.csv\")\n",
    "    json_file_path = os.path.join(base_dir, \"tweet_ids.json\")\n",
    "    df = pd.read_csv(srw_split_file_path, sep=\",\", header=None, names=[\"tweet_id\", \"label\"])\n",
    "    print(df.shape)\n",
    "    \n",
    "    # Load CSS data\n",
    "    df_css = pd.read_csv(css_split_file_path, sep=\"\\t\", usecols=[\"TweetID\"]).reset_index()\n",
    "    df_css = df_css.rename(columns={\n",
    "        \"index\": \"tweet_id\",\n",
    "        \"TweetID\": \"label\"\n",
    "    })\n",
    "    print(df_css.shape)\n",
    "    # Rename label\n",
    "    df_css.loc[df_css.label == \"neither\", \"label\"] = \"none\"\n",
    "    # Drop both label as too ambigous\n",
    "    df_css = df_css.drop(df_css[df_css.label == \"both\"].index, axis=0)\n",
    "    print(df_css.shape)\n",
    "    \n",
    "    # Concat both data\n",
    "    df = pd.concat([df, df_css], axis=0)\n",
    "    print(df.shape)\n",
    "    df = df.drop_duplicates(subset=[\"tweet_id\", \"label\"])\n",
    "    print(df.shape)\n",
    "    \n",
    "    # Join with text\n",
    "    df_json = tweet_json_to_df(json_file_path)\n",
    "    print(df_json.shape)\n",
    "    df = df.merge(df_json, on=\"tweet_id\", how=\"inner\")\n",
    "    print(df.shape)\n",
    "    df[\"dataset\"] = \"WaseemSRW\"\n",
    "    df[\"datasplit\"] = \"train\"\n",
    "    df = split_data(df)\n",
    "    return df\n",
    "\n",
    "HATESPEECH_SRW_BASE_DIR = \"G:\\\\AzureBackup\\\\Datasets\\\\Twitter\\\\hatespeech\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16907, 2)\n",
      "(6909, 2)\n",
      "(6859, 2)\n",
      "(23766, 2)\n",
      "(19752, 2)\n",
      "(18172, 2)\n",
      "(18295, 3)\n",
      "Index(['tweet_id', 'label', 'text', 'dataset', 'datasplit'], dtype='object')\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr:last-of-type th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th>datasplit</th>\n",
       "      <th colspan=\"3\" halign=\"left\">dev</th>\n",
       "      <th colspan=\"3\" halign=\"left\">test</th>\n",
       "      <th colspan=\"3\" halign=\"left\">train</th>\n",
       "      <th>All</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>label</th>\n",
       "      <th>none</th>\n",
       "      <th>racism</th>\n",
       "      <th>sexism</th>\n",
       "      <th>none</th>\n",
       "      <th>racism</th>\n",
       "      <th>sexism</th>\n",
       "      <th>none</th>\n",
       "      <th>racism</th>\n",
       "      <th>sexism</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dataset</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>WaseemSRW</th>\n",
       "      <td>1007</td>\n",
       "      <td>159</td>\n",
       "      <td>298</td>\n",
       "      <td>2516</td>\n",
       "      <td>399</td>\n",
       "      <td>744</td>\n",
       "      <td>9057</td>\n",
       "      <td>1437</td>\n",
       "      <td>2678</td>\n",
       "      <td>18295</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>All</th>\n",
       "      <td>1007</td>\n",
       "      <td>159</td>\n",
       "      <td>298</td>\n",
       "      <td>2516</td>\n",
       "      <td>399</td>\n",
       "      <td>744</td>\n",
       "      <td>9057</td>\n",
       "      <td>1437</td>\n",
       "      <td>2678</td>\n",
       "      <td>18295</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "datasplit   dev                test               train                  All\n",
       "label      none racism sexism  none racism sexism  none racism sexism       \n",
       "dataset                                                                     \n",
       "WaseemSRW  1007    159    298  2516    399    744  9057   1437   2678  18295\n",
       "All        1007    159    298  2516    399    744  9057   1437   2678  18295"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = load_hatespeech_srw_data(HATESPEECH_SRW_BASE_DIR)\n",
    "print(df.columns)\n",
    "df.pivot_table(index=\"dataset\", columns=[\"datasplit\", \"label\"], values=\"tweet_id\", aggfunc=len, margins=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data: WaseemSRW, split: dev, shape: (1464, 5), written to ../data/processed/ABUSIVE\\WaseemSRW\\dev.json\n",
      "Data: WaseemSRW, split: test, shape: (3659, 5), written to ../data/processed/ABUSIVE\\WaseemSRW\\test.json\n",
      "Data: WaseemSRW, split: train, shape: (13172, 5), written to ../data/processed/ABUSIVE\\WaseemSRW\\train.json\n"
     ]
    }
   ],
   "source": [
    "write_data(df, ABUSIVE_WRITE_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parse Veridicality data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TOO SMALL DATA DO NOT USE\n",
    "\n",
    "VERIDICALITY_LABEL_MAP = {\n",
    "    \"DY\": \"definitely_yes\",\n",
    "    \"DN\": \"definitely_no\",\n",
    "    \"PY\": \"probably_yes\",\n",
    "    \"PN\": \"probably_no\",\n",
    "    \"UC\": \"uncertain\",\n",
    "}\n",
    "\n",
    "def load_veridicality_data(base_dir):\n",
    "    split_file_path = os.path.join(base_dir, \"veridicality_data.csv\")\n",
    "    json_file_path = os.path.join(base_dir, \"tweet_ids.txt.json.gz\")\n",
    "    df = pd.read_csv(split_file_path, sep=\",\")\n",
    "    df = df.rename(columns={\n",
    "        \"TweetID\": \"tweet_id\", \n",
    "        \" Annotation\": \"label\"\n",
    "    })\n",
    "    print(df.label.value_counts())\n",
    "    df.label = df.label.map(VERIDICALITY_LABEL_MAP.get)\n",
    "    print(df.shape)\n",
    "    df_json = tweet_json_to_df(json_file_path)\n",
    "    print(df_json.shape)\n",
    "    df = df.merge(df_json, on=\"tweet_id\", how=\"inner\")\n",
    "    print(df.shape)\n",
    "    df[\"dataset\"] = \"Swamy\"\n",
    "    df[\"datasplit\"] = \"train\"\n",
    "    df = split_data(df)\n",
    "    return df\n",
    "\n",
    "VERIDICALITY_BASE_DIR = \"G:\\\\AzureBackup\\\\Datasets\\\\Twitter\\\\Twitter-Veridicality\\\\data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UC    390\n",
      "DY    284\n",
      "PY    224\n",
      "PN     65\n",
      "DN     34\n",
      "Name: label, dtype: int64\n",
      "(997, 2)\n",
      "(237442, 2)\n",
      "(911, 3)\n",
      "Index(['tweet_id', 'label', 'text', 'dataset', 'datasplit'], dtype='object')\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr:last-of-type th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th>datasplit</th>\n",
       "      <th colspan=\"5\" halign=\"left\">dev</th>\n",
       "      <th colspan=\"5\" halign=\"left\">test</th>\n",
       "      <th colspan=\"5\" halign=\"left\">train</th>\n",
       "      <th>All</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>label</th>\n",
       "      <th>definitely_no</th>\n",
       "      <th>definitely_yes</th>\n",
       "      <th>probably_no</th>\n",
       "      <th>probably_yes</th>\n",
       "      <th>uncertain</th>\n",
       "      <th>definitely_no</th>\n",
       "      <th>definitely_yes</th>\n",
       "      <th>probably_no</th>\n",
       "      <th>probably_yes</th>\n",
       "      <th>uncertain</th>\n",
       "      <th>definitely_no</th>\n",
       "      <th>definitely_yes</th>\n",
       "      <th>probably_no</th>\n",
       "      <th>probably_yes</th>\n",
       "      <th>uncertain</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dataset</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Swamy</th>\n",
       "      <td>2</td>\n",
       "      <td>20</td>\n",
       "      <td>5</td>\n",
       "      <td>17</td>\n",
       "      <td>29</td>\n",
       "      <td>6</td>\n",
       "      <td>51</td>\n",
       "      <td>12</td>\n",
       "      <td>42</td>\n",
       "      <td>72</td>\n",
       "      <td>23</td>\n",
       "      <td>182</td>\n",
       "      <td>45</td>\n",
       "      <td>148</td>\n",
       "      <td>257</td>\n",
       "      <td>911</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>All</th>\n",
       "      <td>2</td>\n",
       "      <td>20</td>\n",
       "      <td>5</td>\n",
       "      <td>17</td>\n",
       "      <td>29</td>\n",
       "      <td>6</td>\n",
       "      <td>51</td>\n",
       "      <td>12</td>\n",
       "      <td>42</td>\n",
       "      <td>72</td>\n",
       "      <td>23</td>\n",
       "      <td>182</td>\n",
       "      <td>45</td>\n",
       "      <td>148</td>\n",
       "      <td>257</td>\n",
       "      <td>911</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "datasplit           dev                                                    \\\n",
       "label     definitely_no definitely_yes probably_no probably_yes uncertain   \n",
       "dataset                                                                     \n",
       "Swamy                 2             20           5           17        29   \n",
       "All                   2             20           5           17        29   \n",
       "\n",
       "datasplit          test                                                    \\\n",
       "label     definitely_no definitely_yes probably_no probably_yes uncertain   \n",
       "dataset                                                                     \n",
       "Swamy                 6             51          12           42        72   \n",
       "All                   6             51          12           42        72   \n",
       "\n",
       "datasplit         train                                                    All  \n",
       "label     definitely_no definitely_yes probably_no probably_yes uncertain       \n",
       "dataset                                                                         \n",
       "Swamy                23            182          45          148       257  911  \n",
       "All                  23            182          45          148       257  911  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = load_veridicality_data(VERIDICALITY_BASE_DIR)\n",
    "print(df.columns)\n",
    "df.pivot_table(index=\"dataset\", columns=[\"datasplit\", \"label\"], values=\"tweet_id\", aggfunc=len, margins=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data: Swamy, split: dev, shape: (73, 5), written to ../data/processed/UNCERTAINITY\\Swamy\\dev.json\n",
      "Data: Swamy, split: test, shape: (183, 5), written to ../data/processed/UNCERTAINITY\\Swamy\\test.json\n",
      "Data: Swamy, split: train, shape: (655, 5), written to ../data/processed/UNCERTAINITY\\Swamy\\train.json\n"
     ]
    }
   ],
   "source": [
    "write_data(df, UNCERTAINITY_WRITE_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parse Sarcasm data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_sarcasm_data(base_dir):\n",
    "    split_file_path = os.path.join(base_dir, \"sarcasm-data\", \"sarcasm-annos-emnlp13.tsv\")\n",
    "    json_file_path = os.path.join(base_dir, \"tweet_ids.txt.json.gz\")\n",
    "    df = pd.read_csv(split_file_path, sep=\"\\t\", header=None, names=[\"tweet_id\", \"label\"])\n",
    "    print(df.shape)\n",
    "    df.label = df.label.str.lower()\n",
    "    df_json = tweet_json_to_df(json_file_path)\n",
    "    print(df_json.shape)\n",
    "    df = df.merge(df_json, on=\"tweet_id\", how=\"inner\")\n",
    "    print(df.shape)\n",
    "    df[\"dataset\"] = \"Riloff\"\n",
    "    df[\"datasplit\"] = \"train\"\n",
    "    df = split_data(df)\n",
    "    return df\n",
    "\n",
    "SARCASM_BASE_DIR = \"G:\\\\AzureBackup\\\\Datasets\\\\Twitter\\\\\\SarcasmRiloff2013\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3000, 2)\n",
      "(1808, 2)\n",
      "(1808, 3)\n",
      "Index(['tweet_id', 'label', 'text', 'dataset', 'datasplit'], dtype='object')\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr:last-of-type th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th>datasplit</th>\n",
       "      <th colspan=\"2\" halign=\"left\">dev</th>\n",
       "      <th colspan=\"2\" halign=\"left\">test</th>\n",
       "      <th colspan=\"2\" halign=\"left\">train</th>\n",
       "      <th>All</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>label</th>\n",
       "      <th>not_sarcasm</th>\n",
       "      <th>sarcasm</th>\n",
       "      <th>not_sarcasm</th>\n",
       "      <th>sarcasm</th>\n",
       "      <th>not_sarcasm</th>\n",
       "      <th>sarcasm</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dataset</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Riloff</th>\n",
       "      <td>115</td>\n",
       "      <td>30</td>\n",
       "      <td>288</td>\n",
       "      <td>74</td>\n",
       "      <td>1033</td>\n",
       "      <td>268</td>\n",
       "      <td>1808</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>All</th>\n",
       "      <td>115</td>\n",
       "      <td>30</td>\n",
       "      <td>288</td>\n",
       "      <td>74</td>\n",
       "      <td>1033</td>\n",
       "      <td>268</td>\n",
       "      <td>1808</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "datasplit         dev                test               train           All\n",
       "label     not_sarcasm sarcasm not_sarcasm sarcasm not_sarcasm sarcasm      \n",
       "dataset                                                                    \n",
       "Riloff            115      30         288      74        1033     268  1808\n",
       "All               115      30         288      74        1033     268  1808"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = load_sarcasm_data(SARCASM_BASE_DIR)\n",
    "print(df.columns)\n",
    "df.pivot_table(index=\"dataset\", columns=[\"datasplit\", \"label\"], values=\"tweet_id\", aggfunc=len, margins=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data: Riloff, split: dev, shape: (145, 5), written to ../data/processed/UNCERTAINITY\\Riloff\\dev.json\n",
      "Data: Riloff, split: test, shape: (362, 5), written to ../data/processed/UNCERTAINITY\\Riloff\\test.json\n",
      "Data: Riloff, split: train, shape: (1301, 5), written to ../data/processed/UNCERTAINITY\\Riloff\\train.json\n"
     ]
    }
   ],
   "source": [
    "write_data(df, UNCERTAINITY_WRITE_DIR)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
